services:
  # Minio - S3 compatible storage
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000" # API
      - "9001:9001" # Console UI
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  # MySQL - Base de données pour metadata
  mysql:
    image: mysql:8.0
    container_name: mysql
    ports:
      - "3306:3306"
    environment:
      MYSQL_ROOT_PASSWORD: rootpassword
      MYSQL_DATABASE: mlops_db
      MYSQL_USER: mlops_user
      MYSQL_PASSWORD: mlops_pass
    volumes:
      - mysql_data:/var/lib/mysql
      - ./init_db.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Airflow Webserver
  airflow-webserver:
    image: apache/airflow:2.8.0
    container_name: airflow-webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    ports:
      - "8080:8080"
    environment:
      _AIRFLOW_DB_MIGRATE: "true"
      _AIRFLOW_WWW_USER_CREATE: "true"
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
      _AIRFLOW_WWW_USER_EMAIL: admin@example.com
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+pymysql://mlops_user:mlops_pass@mysql:3306/mlops_db
      AIRFLOW__CORE__FERNET_KEY: ""
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth"
      AIRFLOW__CONNECTION__AWS_DEFAULT__CONN_TYPE: s3
      AIRFLOW__CONNECTION__AWS_DEFAULT__HOST: http://minio:9000
      AIRFLOW__CONNECTION__AWS_DEFAULT__LOGIN: minioadmin
      AIRFLOW__CONNECTION__AWS_DEFAULT__PASSWORD: minioadmin
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    entrypoint: /bin/bash
    command:
      - -c
      - |
        pip install --quiet --no-cache-dir apache-airflow-providers-mysql pymysql
        exec /entrypoint webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Airflow Scheduler
  airflow-scheduler:
    image: apache/airflow:2.8.0
    container_name: airflow-scheduler
    depends_on:
      airflow-webserver:
        condition: service_healthy
    environment:
      _AIRFLOW_DB_MIGRATE: "true"
      _AIRFLOW_WWW_USER_CREATE: "true"
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
      _AIRFLOW_WWW_USER_EMAIL: admin@example.com
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+pymysql://mlops_user:mlops_pass@mysql:3306/mlops_db
      AIRFLOW__CORE__FERNET_KEY: ""
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth"
      AIRFLOW__CONNECTION__AWS_DEFAULT__CONN_TYPE: s3
      AIRFLOW__CONNECTION__AWS_DEFAULT__HOST: http://minio:9000
      AIRFLOW__CONNECTION__AWS_DEFAULT__LOGIN: minioadmin
      AIRFLOW__CONNECTION__AWS_DEFAULT__PASSWORD: minioadmin
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    entrypoint: /bin/bash
    command:
      - -c
      - |
        pip install --quiet --no-cache-dir apache-airflow-providers-mysql pymysql
        exec /entrypoint scheduler
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"',
        ]
      interval: 30s
      timeout: 10s
      retries: 5

  # Airflow Init (run once)
  airflow-init:
    image: apache/airflow:2.8.0
    container_name: airflow-init
    depends_on:
      mysql:
        condition: service_healthy
    environment:
      _AIRFLOW_DB_MIGRATE: "true"
      _AIRFLOW_WWW_USER_CREATE: "true"
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
      _AIRFLOW_WWW_USER_EMAIL: admin@example.com
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+pymysql://mlops_user:mlops_pass@mysql:3306/mlops_db
      AIRFLOW__CORE__FERNET_KEY: ""
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    entrypoint: /bin/bash
    command:
      - -c
      - |
        echo "Installation du provider MySQL pour Airflow..."
        pip install --quiet --no-cache-dir apache-airflow-providers-mysql pymysql
        echo "Vérification installation..."
        python -c "import pymysql; print('✅ pymysql installé')" || echo "⚠️  pymysql non trouvé"
        echo "Attente de MySQL..."
        sleep 15
        echo "Tentative de migration..."
        for i in 1 2 3 4 5; do
          if airflow db migrate 2>&1 | tee /tmp/migrate.log; then
            echo "✅ Migration réussie!"
            break
          else
            if grep -q "already exists\|already initialized\|Upgrade done" /tmp/migrate.log 2>/dev/null; then
              echo "✅ Base déjà initialisée"
              break
            fi
            echo "Tentative $$i/5 échouée, nouvelle tentative dans 10 secondes..."
            sleep 10
          fi
        done
        echo "Création de l'utilisateur admin..."
        airflow users create --username admin --firstname admin --lastname admin --role Admin --email admin@example.com --password admin 2>&1 || echo "⚠️  Utilisateur admin existe déjà ou erreur"
        echo "✅ Initialisation Airflow terminée !"

  # Prometheus - Monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:9090/-/healthy",
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  # Grafana - Monitoring Dashboard
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_USERS_ALLOW_SIGN_UP: false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - prometheus
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --quiet --tries=1 --spider http://localhost:3000/api/health || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  minio_data:
  mysql_data:
  prometheus_data:
  grafana_data:
