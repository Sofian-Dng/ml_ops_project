# ğŸš€ Pipeline MLOps - Classification Binaire d'Images

**Classification pissenlit vs herbe avec pipeline MLOps **

## ğŸ‘¥ Ã‰quipe

**Membres du projet :**
- Sofian Duong
- Joseph Dejean
- Maxandre Michel
- Paul Montier
- Mathieu Chabirand
- Chemseddine nadour
- emre

## ğŸ“Š Stack Technique

- **ModÃ¨le**: TensorFlow/Keras (CNN)
- **Tracking & API**: MLflow
- **Storage S3**: Minio (S3 compatible)
- **Feature Store**: Parquet + MySQL
- **Orchestration**: Apache Airflow
- **Conteneurisation**: Docker
- **DÃ©ploiement**: Kubernetes (2 pods)
- **CI/CD**: GitHub Actions
- **Monitoring**: Prometheus + Grafana
- **Interface**: Gradio


## ğŸ“ Structure du Projet

```
mlops-project-git/
â”œâ”€â”€ NOTEBOOK_PRESENTATION_FINAL.ipynb  # Notebook prÃ©sentation
â”œâ”€â”€ download_data.py                   # TÃ©lÃ©chargement images
â”œâ”€â”€ train.py                           # EntraÃ®nement modÃ¨le
â”œâ”€â”€ gradio_app.py                      # Interface web
â”œâ”€â”€ utils_s3.py                        # Client Minio/S3
â”œâ”€â”€ feature_store.py                   # Feature Store
â”œâ”€â”€ requirements.txt                   # DÃ©pendances Python
â”œâ”€â”€ Dockerfile                         # Image Docker (local)
â”œâ”€â”€ Dockerfile.s3                      # Image Docker (depuis S3)
â”œâ”€â”€ entrypoint.sh                      # Script dÃ©marrage Docker
â”œâ”€â”€ entrypoint_s3.sh                   # Script dÃ©marrage Docker S3
â”œâ”€â”€ docker-compose.yml                 # Services (Minio, Airflow, Monitoring)
â”œâ”€â”€ init_db.sql                        # Initialisation MySQL
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ deployment.yaml                # Deployment Kubernetes
â”‚   â””â”€â”€ service.yaml                   # Service Kubernetes
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ mlops_retraining_pipeline.py
â”‚       â””â”€â”€ continuous_training_dag.py
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â””â”€â”€ grafana/
â”‚       â”œâ”€â”€ datasources/prometheus.yml
â”‚       â””â”€â”€ dashboards/
â”‚           â”œâ”€â”€ dashboard.json
â”‚           â””â”€â”€ mlops_dashboard.json
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ mlops-pipeline.yml         # CI/CD GitHub Actions
```



### 1. DÃ©pendances Python

```bash
pip install -r requirements.txt
```

### 2. Services Docker Compose

```bash
# DÃ©marrer tous les services
docker-compose up -d

# Services disponibles:
# - Minio: http://localhost:9001 (minioadmin/minioadmin)
# - Airflow: http://localhost:8080 (admin/admin)
# - Prometheus: http://localhost:9090
# - Grafana: http://localhost:3000 (admin/admin)
# - MLflow UI: http://localhost:5001
# - MySQL: localhost:3306

# Pour les mÃ©triques Prometheus, lancer aussi :
python generate_prometheus_metrics.py
```

## ğŸ“‹ Utilisation

### Phase 1 : DonnÃ©es et ModÃ¨le

```bash
# 1. TÃ©lÃ©charger les donnÃ©es (400 images)
python download_data.py

# 2. EntraÃ®ner le modÃ¨le (5-10 minutes)
python train.py
```

**RÃ©sultat** :
- ModÃ¨le enregistrÃ© dans `mlruns/` (MLflow)
- ModÃ¨le uploadÃ© vers Minio/S3 (bucket `mlops-models`)
- Features extraites et stockÃ©es dans Feature Store (Parquet + MySQL)
  - Parquet : `feature_store/features_*.parquet`
  - MySQL : Table `feature_store` avec mÃ©tadonnÃ©es

### Phase 2 : Docker

```bash
# Build l'image Docker
docker build -t dandelion-grass-classifier:latest .

# Tester localement
docker run -p 5000:5000 dandelion-grass-classifier:latest
```

**API accessible** : http://localhost:5000/invocations (POST uniquement)

**Note** : Le container Docker s'appelle `mlflow-model-api` (renommÃ© depuis `elegant_agnesi`)

### Phase 3 : Kubernetes

```bash
# DÃ©ployer
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml

# VÃ©rifier
kubectl get pods
kubectl get services
```

**API accessible** : http://localhost:30080/invocations (POST uniquement)

**Note** : Les endpoints `/invocations` n'acceptent que POST. Un GET retournera `405 Method Not Allowed`.

### Phase 4 : Airflow

1. Ouvrir http://localhost:8080 (admin/admin)
2. **Note** : Les DAGs sont en pause par dÃ©faut (sÃ©curitÃ©)
3. Pour activer : Toggle le switch Ã  cÃ´tÃ© du DAG ou utiliser le bouton Play pour exÃ©cution manuelle
4. DAGs disponibles :
   - `mlops_retraining_pipeline` : Pipeline complet (toutes les 7 jours)
   - `continuous_training` : VÃ©rification continue (toutes les 6 heures)

### Phase 5 : Interface Gradio

```bash
python gradio_app.py
```

**Interface accessible** : http://localhost:7860

## ğŸ“Š Notebook de PrÃ©sentation

Ouvrir `NOTEBOOK_PRESENTATION_FINAL.ipynb` pour :
- Vue d'ensemble du projet
- Tests exÃ©cutables pour chaque phase
- DÃ©monstration complÃ¨te

## ğŸ”— URLs d'AccÃ¨s

| Service | URL | Credentials | Notes |
|---------|-----|------------|-------|
| ğŸ¨ Gradio | http://localhost:7860 | - | Interface web interactive |
| â˜¸ï¸ API K8s | http://localhost:30080/invocations | - | POST uniquement (2 pods) |
| ğŸ³ API Docker | http://localhost:5000/invocations | - | POST uniquement, container: `mlflow-model-api` |
| ğŸ“Š MLflow UI | http://localhost:5001 | - | Via docker-compose (statut peut Ãªtre "unhealthy") |
| ğŸ”„ Airflow | http://localhost:8080 | admin/admin | DAGs en pause par dÃ©faut |
| ğŸ“Š Prometheus | http://localhost:9090 | - | Collecte mÃ©triques |
| ğŸ“ˆ Grafana | http://localhost:3000 | admin/admin | Dashboards monitoring |
| ğŸ’¾ Minio Console | http://localhost:9001 | minioadmin/minioadmin | Stockage S3 |

**âš ï¸ Notes importantes** :
- Les APIs `/invocations` n'acceptent que **POST** (GET = 405 Method Not Allowed)
- Les DAGs Airflow sont **en pause par dÃ©faut** (sÃ©curitÃ©) â†’ Activer avec toggle ou Play
- Le script `generate_prometheus_metrics.py` doit Ãªtre **en cours d'exÃ©cution** pour les mÃ©triques
- MLflow UI peut Ãªtre "unhealthy" mais fonctionne quand mÃªme sur http://localhost:5001

## ğŸ› ï¸ Commandes Essentielles

```bash
# DÃ©marrer tous les services
docker-compose up -d

# EntraÃ®ner le modÃ¨le (avec upload S3 et Feature Store)
python train.py

# Lancer l'interface Gradio
python gradio_app.py

# VÃ©rifier les pods Kubernetes
kubectl get pods -l app=dandelion-grass-classifier

# Voir les logs d'un pod
kubectl logs <pod-name>

# Lancer le gÃ©nÃ©rateur de mÃ©triques (pour Prometheus)
python generate_prometheus_metrics.py

# ArrÃªter tous les services
docker-compose down
```

## ğŸ“Š Monitoring

### Prometheus et Grafana

Le projet inclut un monitoring complet avec Prometheus (collecte) et Grafana (visualisation).

#### GÃ©nÃ©ration de mÃ©triques

```bash
# Lancer le gÃ©nÃ©rateur de mÃ©triques de dÃ©monstration
python generate_prometheus_metrics.py
```

**Important** : Gardez ce script en cours d'exÃ©cution pour que Prometheus puisse collecter les mÃ©triques.

#### AccÃ¨s aux interfaces

- **Prometheus** : http://localhost:9090
  - VÃ©rifier les targets : Status â†’ Targets
  - RequÃªtes : Graph â†’ Tapez `mlops_api_requests_total`
  
- **Grafana** : http://localhost:3000 (admin/admin)
  - CrÃ©er des dashboards avec les mÃ©triques Prometheus
  - RequÃªtes utiles : `mlops_model_predictions_total`, `mlops_model_confidence`, `rate(mlops_api_requests_total[5m])`

#### MÃ©triques disponibles

- `mlops_api_requests_total` : Nombre total de requÃªtes API
- `mlops_model_predictions_total` : PrÃ©dictions par classe
- `mlops_model_confidence` : Confiance du modÃ¨le
- `mlops_kubernetes_pods` : Nombre de pods actifs
- `mlops_api_request_duration_seconds` : DurÃ©e des requÃªtes

## ğŸ“ Choix Techniques et Justifications

### Pourquoi ces outils ?

#### **TensorFlow/Keras**
- **Choix** : Framework de deep learning standard et bien documentÃ©
- **Avantage** : IntÃ©gration native avec MLflow, support complet de SavedModel

#### **MLflow**
- **Choix** : Solution open-source pour le tracking et versioning de modÃ¨les
- **Avantage** : Tracking automatique des mÃ©triques, versioning, API REST intÃ©grÃ©e (`mlflow models serve`)

#### **Minio (S3 compatible)**
- **Choix** : Stockage objet compatible S3 pour stocker les modÃ¨les
- **Avantage** : Facile Ã  dÃ©ployer localement, compatible avec boto3, migration vers AWS S3 transparente

#### **Apache Airflow**
- **Choix** : Orchestrateur de workflows open-source standard
- **Avantage** : DAGs visuels, scheduling flexible, gestion d'erreurs robuste

#### **Docker**
- **Choix** : Conteneurisation standard pour isoler les dÃ©pendances
- **Avantage** : ReproducibilitÃ©, portabilitÃ©, isolation des dÃ©pendances

#### **Kubernetes**
- **Choix** : Orchestration de conteneurs pour haute disponibilitÃ©
- **Avantage** : ScalabilitÃ© automatique, 2 pods pour haute disponibilitÃ©, load balancing

#### **Prometheus + Grafana**
- **Choix** : Stack de monitoring standard dans l'industrie
- **Avantage** : MÃ©triques temps rÃ©el, dashboards personnalisables, alerting
- **RÃ´le** : Prometheus collecte les mÃ©triques, Grafana les visualise

#### **Gradio**
- **Choix** : Interface web interactive rapide Ã  dÃ©velopper
- **Avantage** : Interface prÃªte en quelques lignes, upload d'images facile

#### **Feature Store (Parquet + MySQL)**
- **Choix** : Stockage de features avec mÃ©tadonnÃ©es
- **Avantage** : Parquet pour performances, MySQL pour mÃ©tadonnÃ©es et requÃªtes
- **ImplÃ©mentation** : 
  - Parquet files pour stockage efficace des features extraites des images
  - MySQL pour mÃ©tadonnÃ©es (nom, chemin, timestamp, run_id)
  - Automatiquement rempli lors de `train.py`

## ğŸ§ª Tests

Le projet inclut une suite de tests complÃ¨te :

### Structure des tests

- **Tests unitaires** (`tests/test_unit.py`) : 10 tests pour les fonctions individuelles
- **Tests d'intÃ©gration** (`tests/test_integration.py`) : 8 tests pour les interactions entre composants
- **Tests end-to-end** (`tests/test_e2e.py`) : 11 tests pour le flux complet du pipeline

### ExÃ©cution des tests

```bash
# Tous les tests
python run_tests.py

# Ou avec pytest directement
python -m pytest tests/ -v

# Par catÃ©gorie
python -m pytest tests/test_unit.py -v
python -m pytest tests/test_integration.py -v
python -m pytest tests/test_e2e.py -v
```

> ğŸ“– Voir `tests/README.md` pour plus de dÃ©tails

## ğŸ“Š RÃ©sultats Obtenus

### MÃ©triques du ModÃ¨le

- **Accuracy d'entraÃ®nement** : ~85-90% (selon les runs)
- **Accuracy de validation** : ~80-85%
- **Format** : Classification binaire (Pissenlit vs Herbe)
- **Taille du modÃ¨le** : ~10-15 MB (SavedModel)

### Performance du Pipeline

- **Temps d'entraÃ®nement** : 5-10 minutes (400 images, 10 epochs)
- **Temps de dÃ©ploiement Docker** : ~2 minutes (build + run)
- **Temps de dÃ©ploiement Kubernetes** : ~1 minute (2 pods)
- **Latence API** : < 500ms par prÃ©diction


## ğŸ³ Docker Hub

### Image Docker disponible

L'image Docker du modÃ¨le est disponible sur Docker Hub :

**URL de l'image :** https://hub.docker.com/r/khal160/dandelion-grass-classifier

âœ… Image publiÃ©e et accessible publiquement sur Docker Hub.

### Pull et utilisation

```bash
# Pull l'image depuis Docker Hub
docker pull khal160/dandelion-grass-classifier:latest

# Lancer le container
docker run -p 5000:5000 khal160/dandelion-grass-classifier:latest
```

### Push vers Docker Hub

```bash
# 1. Se connecter Ã  Docker Hub
docker login

# 2. Tag l'image
docker tag dandelion-grass-classifier:latest khal160/dandelion-grass-classifier:latest

# 3. Push l'image
docker push khal160/dandelion-grass-classifier:latest
```

## ğŸ“ Notes Techniques

- **ModÃ¨le** : CNN simple (3 couches convolutionnelles) pour classification binaire
- **Format API** : JSON avec `{"inputs": [[image_normalisÃ©e_224x224x3]]}`
- **MLflow** : Tracking automatique des mÃ©triques et versioning du modÃ¨le
  - **UI** : http://localhost:5001 (via docker-compose)
  - Statut peut Ãªtre "unhealthy" mais fonctionne quand mÃªme
- **Docker** : Utilise `mlflow models serve` (pas besoin de FastAPI)
  - Container nommÃ© `mlflow-model-api` sur port 5000
- **Kubernetes** : 2 pods pour haute disponibilitÃ©, NodePort 30080
- **APIs** : Les endpoints `/invocations` n'acceptent que POST (GET = 405)
- **Airflow** : DAGs en pause par dÃ©faut (sÃ©curitÃ©), activer avec toggle ou Play
- **Monitoring** : `generate_prometheus_metrics.py` doit Ãªtre en cours d'exÃ©cution
- **CI/CD** : Workflow GitHub Actions dÃ©clenchÃ© sur push vers `main`
- **Tests** : Suite complÃ¨te de tests unitaires, intÃ©gration et E2E

## ğŸ”„ CI/CD Pipeline

Le pipeline CI/CD GitHub Actions :

1. **Checkout** du code
2. **Installation** des dÃ©pendances Python
3. **TÃ©lÃ©chargement** des donnÃ©es d'entraÃ®nement
4. **EntraÃ®nement** du modÃ¨le avec MLflow
5. **Build** de l'image Docker
6. **DÃ©ploiement** (optionnel, selon configuration)

> ğŸ“– Voir `.github/workflows/mlops-pipeline.yml` pour les dÃ©tails


